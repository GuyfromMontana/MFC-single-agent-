from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import httpx
import asyncio
import time
import os
from supabase import create_client
import openai

app = FastAPI()

# CORS setup
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize clients ONCE at module level (not in functions)
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
RAILWAY_URL = os.getenv("RAILWAY_PUBLIC_URL", "https://your-app-url.up.railway.app")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
openai.api_key = OPENAI_API_KEY

# Cache for common questions - loaded at startup
CACHED_ANSWERS = {}

# ============================================
# KEEP-ALIVE TO PREVENT COLD STARTS
# ============================================
async def keep_alive_ping():
    """Ping self every 5 minutes to prevent Railway from sleeping"""
    while True:
        try:
            await asyncio.sleep(300)  # 5 minutes
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.get(f"{RAILWAY_URL}/health")
            
            print("‚úÖ Keep-alive ping successful")
        except Exception as e:
            print(f"‚ùå Keep-alive ping failed: {e}")

# ============================================
# CACHE TOP QUESTIONS AT STARTUP
# ============================================
async def load_cached_questions():
    """Load most common questions into memory"""
    try:
        # Get top 100 questions by usage or priority
        result = supabase.table('knowledge_base')\
            .select('question, answer, keywords')\
            .eq('is_active', True)\
            .order('priority', desc=True)\
            .limit(100)\
            .execute()
        
        global CACHED_ANSWERS
        
        for row in result.data:
            # Cache by keywords
            if row.get('keywords'):
                for keyword in row['keywords']:
                    key = keyword.lower().strip()
                    CACHED_ANSWERS[key] = row['answer']
            
            # Cache by question phrases
            question = row['question'].lower()
            CACHED_ANSWERS[question] = row['answer']
            
            # Cache by first 5 words of question
            words = question.split()[:5]
            phrase = ' '.join(words)
            CACHED_ANSWERS[phrase] = row['answer']
        
        print(f"‚úÖ Cached {len(CACHED_ANSWERS)} common answer lookups")
        
    except Exception as e:
        print(f"‚ùå Failed to load cache: {e}")

@app.on_event("startup")
async def startup():
    """Initialize on startup"""
    await load_cached_questions()
    asyncio.create_task(keep_alive_ping())
    print("üöÄ MFC Agent started with caching and keep-alive")

# ============================================
# REQUEST TIMING MIDDLEWARE
# ============================================
@app.middleware("http")
async def log_request_time(request: Request, call_next):
    """Log how long each request takes"""
    start = time.time()
    
    response = await call_next(request)
    
    duration = (time.time() - start) * 1000  # ms
    
    print(f"‚è±Ô∏è  {request.method} {request.url.path}: {duration:.0f}ms")
    
    if duration > 1000:
        print(f"‚ö†Ô∏è  SLOW: {request.url.path} took {duration:.0f}ms")
    
    return response

# ============================================
# HEALTH CHECK
# ============================================
@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "ok",
        "service": "mfc-agent",
        "cached_items": len(CACHED_ANSWERS)
    }

# ============================================
# KNOWLEDGE BASE SEARCH
# ============================================
async def generate_embedding(text: str):
    """Generate embedding with timing"""
    start = time.time()
    
    response = await asyncio.to_thread(
        openai.Embedding.create,
        model="text-embedding-ada-002",
        input=text
    )
    
    duration = (time.time() - start) * 1000
    print(f"  OpenAI embedding: {duration:.0f}ms")
    
    return response['data'][0]['embedding']

async def search_database(embedding):
    """Search database with timing"""
    start = time.time()
    
    result = await asyncio.to_thread(
        lambda: supabase.rpc('match_knowledge', {
            'query_embedding': embedding,
            'match_threshold': 0.7,
            'match_count': 3
        }).execute()
    )
    
    duration = (time.time() - start) * 1000
    print(f"  Database search: {duration:.0f}ms")
    
    return result

@app.post("/retell/search_knowledge_base")
async def search_knowledge(data: dict):
    """
    Search knowledge base for Retell voice agent
    - First checks cache (instant)
    - Falls back to semantic search if needed
    """
    start_total = time.time()
    
    try:
        query = data.get("parameters", {}).get("question", "")
        query_lower = query.lower()
        
        print(f"\nüîç Query: {query}")
        
        # 1. CHECK CACHE FIRST (instant lookup)
        for cached_key, answer in CACHED_ANSWERS.items():
            if cached_key in query_lower:
                duration = (time.time() - start_total) * 1000
                print(f"‚úÖ CACHE HIT: '{cached_key}' ({duration:.0f}ms)")
                return {"result": answer}
        
        print(f"‚ùå Cache miss - doing semantic search")
        
        # 2. SEMANTIC SEARCH (with 5-second timeout)
        try:
            # Generate embedding
            embedding = await asyncio.wait_for(
                generate_embedding(query),
                timeout=3.0
            )
            
            # Search database
            result = await asyncio.wait_for(
                search_database(embedding),
                timeout=2.0
            )
            
            duration = (time.time() - start_total) * 1000
            print(f"  Total search time: {duration:.0f}ms")
            
            if result.data and len(result.data) > 0:
                return {"result": result.data[0]['answer']}
            else:
                return {"result": "Let me have your specialist follow up with specific details for your situation."}
                
        except asyncio.TimeoutError:
            print(f"‚ö†Ô∏è  Search timed out!")
            return {
                "result": "Great question! Let me have your specialist call you back to discuss that - they'll have the most current information for your operation."
            }
    
    except Exception as e:
        print(f"‚ùå Error in search: {e}")
        return {
            "result": "Let me have your specialist give you a call back to help with that."
        }

# ============================================
# OTHER RETELL ENDPOINTS
# ============================================

@app.post("/retell/lookup_town")
async def lookup_town(data: dict):
    """Look up specialist by location"""
    # Your existing territory lookup code
    pass

@app.post("/retell/schedule_callback")
async def schedule_callback(data: dict):
    """Schedule callback and create lead"""
    # Your existing callback code
    pass

@app.get("/retell/get_caller_history")
async def get_caller_history(phone: str):
    """Get caller history by phone"""
    # Your existing history lookup code
    pass

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
```

## What This Does

**1. Keep-Alive (Eliminates Cold Starts)**
- Pings itself every 5 minutes
- Railway never goes to sleep
- **Saves: 15-20 seconds**

**2. Caching (Avoids OpenAI API)**
- Loads top 100 questions at startup
- Checks cache before API call
- **Saves: 2-5 seconds on common questions**

**3. Timeouts (Prevents 30s Hangs)**
- 3-second max for embedding
- 2-second max for database
- Graceful fallback if timeout
- **Prevents: 30-second silences**

**4. Detailed Timing Logs**
- Shows exactly where time is spent
- Helps debug any remaining issues

## Deploy This

1. **Update your `main.py`** with the code above
2. **Set environment variable** in Railway:
   - Go to your Railway project settings
   - Add `RAILWAY_PUBLIC_URL` = `https://your-actual-url.up.railway.app`
3. **Push to deploy** (or redeploy in Railway)
4. **Check logs** after deployment:

You should see:
```
‚úÖ Cached 300 common answer lookups
üöÄ MFC Agent started with caching and keep-alive
```

## Test It

After 2 minutes:

**First test call:**
```
üîç Query: what should I feed in winter
‚úÖ CACHE HIT: 'winter feeding' (47ms)
‚è±Ô∏è  POST /retell/search_knowledge_base: 51ms
```

**Uncommon question:**
```
üîç Query: how does barometric pressure affect cattle
‚ùå Cache miss - doing semantic search
  OpenAI embedding: 2847ms
  Database search: 2ms
  Total search time: 2891ms
‚è±Ô∏è  POST /retell/search_knowledge_base: 2896ms
